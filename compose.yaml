services:
  # Creates ./speech.env on the host before other services start
  init-env:
    image: alpine:3.20
    working_dir: /work
    volumes:
      - ./:/work
    command: >
      sh -c 'if [ ! -f speech.env ]; then
        if [ -f sample.env ]; then
          cp sample.env speech.env;
          echo "Copied sample.env to speech.env";
        else
          echo "# Auto-created by compose init" > speech.env;
          echo "# Add values like: MUX_TOKEN_ID=value" >> speech.env;
          echo "Created placeholder speech.env";
        fi
      else
        echo "speech.env already exists; leaving it unchanged";
      fi'
    restart: "no"

  #create ollama with GPU Support
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    env_file:
      - ./speech.env
    environment:
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-3}
      - OLLAMA_SCHED_SPREAD=${OLLAMA_SCHED_SPREAD:-1}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # or specify a number like 1
              capabilities: [ gpu ]
    restart: unless-stopped
    #recommend pulling in this model for working stack but not enabled by default
    #entrypoint: >
      #sh -c "
      #(
       # echo 'Pulling model...';
       # ollama serve --cors
       #ollama pull gpt-oss:20b;
       #echo 'Warming up model...';
       #ollama run gpt-oss:20b 'Hello' > /dev/null;
      #) &
      #ollama serve
      #"
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 10s

  #create open-webui with gpu support
  open-webui:
    image: ghcr.io/open-webui/open-webui:cuda
    container_name: open-webui
    depends_on:
      - ollama
      - init-env
    ports:
      - "3000:8080"
    env_file:
      - ./speech.env
    volumes:
      - open-webui-data:/app/backend/data
    restart: unless-stopped

#SETUP MCPO
  mcpo:
    image: ghcr.io/open-webui/mcpo:latest
    container_name: mcpo
    ports:
      - "8001:8001"
    volumes:
      - ./mcpo_config.json:/app/mcpo/config.json
    command: mcpo --port 8001 --config /app/mcpo/config.json --api-key=${MCPO_API_KEY}
    env_file:
      - ./speech.env
    environment:
      - LOG_LEVEL=info
    depends_on:
      - init-env
    restart: unless-stopped

  #SETUP SPEACHES WITH GPU SUPPORT
  speaches:
    image: ghcr.io/speaches-ai/speaches:latest-cuda
    restart: unless-stopped
    ports:
      - 8000:8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    env_file:
      - ./speech.env
    environment:
      - TTS_HOME=/voices
      - HF_HOME=/voices
      # Removed PATH override; let the image manage its own PATH
    volumes:
      - voices:/voices
    depends_on:
      - init-env
    # Removed custom command; use image's default entrypoint/CMD

  # One-off init job to pre-download models after speaches is up
  speaches-init:
    image: alpine:3.20
    depends_on:
      - speaches
    environment:
      - TTS_HOME=/voices
      - HF_HOME=/voices
      - PATH=/root/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    volumes:
      - voices:/voices
    command: >
      sh -lc '
        set -e
        apk add --no-cache curl ca-certificates >/dev/null
        # Wait for speaches API to be reachable
        for i in $(seq 1 90); do
          curl -sfS --connect-timeout 1 http://speaches:8000/ >/dev/null && break || sleep 1
        done
        # Install uv and use uvx to run speaches-cli without polluting the base images
        curl -LsSf https://astral.sh/uv/install.sh | sh
        export PATH="/root/.local/bin:${PATH}"
        uvx --version
        uvx speaches-cli model download Systran/faster-distil-whisper-small.en || true
        uvx speaches-cli model download speaches-ai/Kokoro-82M-v1.0-ONNX || true
        echo "speaches-init: model downloads complete"
      '
    restart: "no"



volumes:
  ollama-data:
  open-webui-data:
  voices:
  config:
